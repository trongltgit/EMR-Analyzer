import os
import threading
import json
import logging
import shutil
import cv2
import numpy as np
from pathlib import Path
import tensorflow as tf
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from PIL import Image
import py7zr  # Th∆∞ vi·ªán ƒë·ªÉ gi·∫£i n√©n .7z
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
from PIL import Image
import urllib.request
import gdown

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.DEBUG,
    filename="server.log",
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Kh·ªüi t·∫°o Flask app
app = Flask(__name__, template_folder="templates", static_folder="static")
CORS(app)  # Cho ph√©p m·ªçi ngu·ªìn g·ªëc truy c·∫≠p



# File ID c·ªßa t·ªáp tr√™n Google Drive
file_id = "your_file_id_here"  # Thay 'your_file_id_here' b·∫±ng ID c·ªßa file

# ƒê∆∞·ªùng d·∫´n t·∫£i xu·ªëng file `best_weights_model.keras`
model_path = "./best_weights_model.keras"

# T·∫£i file t·ª´ Google Drive n·∫øu ch∆∞a t·ªìn t·∫°i
if not os.path.exists(model_path):
    url = f"https://drive.google.com/uc?id={file_id}"
    print(f"Downloading model file from Google Drive ID: {file_id}")
    gdown.download(url, model_path, quiet=False)
    print(f"Model file downloaded successfully to {model_path}")

# Load m√¥ h√¨nh t·ª´ file ƒë√£ t·∫£i xu·ªëng
best_model = load_model(model_path)
print("Model loaded successfully!")








# ƒê∆∞·ªùng d·∫´n v√† bi·∫øn to√†n c·ª•c
MODEL_DIR = "./models"
ASSEMBLED_MODEL = os.path.join(MODEL_DIR, "best_weights_model.7z")
MODEL_PATH = os.path.join(MODEL_DIR, "best_weights_model.keras")
MODEL_PARTS = [
    os.path.join(MODEL_DIR, f"best_weights_model.7z.{str(i).zfill(3)}")
    for i in range(1, 5)
]
model = None

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)
    logging.info(f"üìÅ Created directory: {MODEL_DIR}")

# H√†m h·ª£p nh·∫•t c√°c t·ªáp .7z
def assemble_model_parts():
    try:
        logging.info(f"Assembling model parts: {MODEL_PARTS}")
        with open(ASSEMBLED_MODEL, 'wb') as assembled_file:
            for part in MODEL_PARTS:
                if not os.path.exists(part):
                    logging.error(f"‚ùå Missing model part: {part}")
                    raise FileNotFoundError(f"Missing part: {part}")
                with open(part, 'rb') as part_file:
                    shutil.copyfileobj(part_file, assembled_file)
        logging.info("‚úÖ Successfully assembled model parts into a single .7z file.")
    except Exception as e:
        logging.error(f"‚ùå Failed to assemble model parts: {e}", exc_info=True)
        raise

# H√†m gi·∫£i n√©n t·ªáp .7z
def extract_model():
    try:
        logging.info(f"Extracting model from {ASSEMBLED_MODEL}")
        with py7zr.SevenZipFile(ASSEMBLED_MODEL, mode='r') as archive:
            archive.extractall(path=MODEL_DIR)
        logging.info("‚úÖ Successfully extracted model .keras file.")
    except Exception as e:
        logging.error(f"‚ùå Failed to extract model: {e}", exc_info=True)
        raise

# H√†m chu·∫©n b·ªã m√¥ h√¨nh
def prepare_model():
    try:
        if not os.path.exists(MODEL_PATH):  # Ch·ªâ th·ª±c hi·ªán n·∫øu file .keras ch∆∞a t·ªìn t·∫°i
            if not os.path.exists(ASSEMBLED_MODEL):
                assemble_model_parts()
            extract_model()
    except Exception as e:
        logging.error(f"‚ùå Error in prepare_model: {e}", exc_info=True)
        raise

# H√†m t·∫£i m√¥ h√¨nh v√†o b·ªô nh·ªõ
def load_model():
    global model
    try:
        prepare_model()
        logging.info("üöÄ Loading model into memory...")
        model = tf.keras.models.load_model(MODEL_PATH)
        logging.info("‚úÖ Model loaded successfully!")
    except Exception as e:
        logging.error(f"‚ùå Error loading model: {e}", exc_info=True)
        raise

# T·∫£i m√¥ h√¨nh khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng
try:
    load_model()
except Exception as e:
    logging.error(f"‚ùå Model initialization failed: {e}")

# Route home
@app.route('/')
def home():
    return render_template('index.html')

# Route dashboard
@app.route('/dashboard')
def dashboard():
    return render_template('dashboard.html')

# Route ki·ªÉm tra tr·∫°ng th√°i server
@app.route('/ping', methods=['GET'])
def ping():
    try:
        return jsonify({'message': 'Server is running!'}), 200
    except Exception as e:
        logging.error(f"‚ùå Error in /ping: {e}", exc_info=True)
        return jsonify({'error': 'Server Ping Failed!'}), 500

# Route ki·ªÉm tra tr·∫°ng th√°i m√¥ h√¨nh
@app.route('/model-status', methods=['GET'])
def model_status():
    try:
        if model is not None:
            return jsonify({'status': 'Model is loaded successfully'}), 200
        else:
            return jsonify({'status': 'Model is not loaded'}), 503
    except Exception as e:
        logging.error(f"‚ùå Error checking model status: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

# Route d·ª± ƒëo√°n
@app.route('/predict', methods=['POST'])
def predict():
    try:
        if model is None:
            logging.warning("Model is not loaded, attempting to reload.")
            load_model()

        # Check if an image file is present in the request
        if 'image' not in request.files:
            logging.error("No image file found in the request.")
            return jsonify({'error': 'No image file provided!'}), 400

        file = request.files['image']
        if file.filename == '':
            logging.error("Empty filename provided in the request.")
            return jsonify({'error': 'Empty filename!'}), 400

        # Process the image
        logging.info("üì∑ Processing image for prediction...")
        try:
            img = Image.open(file).convert('RGB').resize((224, 224))
            img_array = np.expand_dims(np.array(img) / 255.0, axis=0)
            img.close()
        except Exception as e:
            logging.error(f"‚ùå Error processing image: {e}", exc_info=True)
            return jsonify({'error': f'Invalid image file: {str(e)}'}), 400

        # Perform prediction
        logging.info("ü§ñ Making prediction...")
        try:
            preds = model.predict(img_array)[0][0]
            classification = 'Nodule' if preds > 0.5 else 'Non-Nodule'
            logging.info(f"‚úÖ Prediction successful: Class - {classification}, Score - {preds}")
            return jsonify({'classification': classification, 'score': float(preds)})
        except Exception as e:
            logging.error(f"‚ùå Error during prediction: {e}", exc_info=True)
            return jsonify({'error': f'Prediction failed: {str(e)}'}), 500

    except Exception as e:
        logging.error(f"‚ùå Unexpected error in /predict: {e}", exc_info=True)
        return jsonify({'error': f'Internal Server Error: {str(e)}'}), 500

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))  # C·ªïng m·∫∑c ƒë·ªãnh l√† 5000 n·∫øu bi·∫øn PORT kh√¥ng t·ªìn t·∫°i
    app.run(host='0.0.0.0', port=port)
