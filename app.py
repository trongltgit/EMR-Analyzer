import os
import logging
import numpy as np
import tensorflow as tf
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from PIL import Image
import py7zr  # Th∆∞ vi·ªán ƒë·ªÉ gi·∫£i n√©n .7z

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.DEBUG,
    filename="server.log",
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Kh·ªüi t·∫°o Flask app
app = Flask(__name__, template_folder="templates", static_folder="static")
CORS(app)  # Cho ph√©p m·ªçi ngu·ªìn g·ªëc truy c·∫≠p

# ƒê∆∞·ªùng d·∫´n v√† bi·∫øn to√†n c·ª•c
MODEL_DIR = "./models"
ASSEMBLED_MODEL = os.path.join(MODEL_DIR, "best_weights_model.7z")
MODEL_PATH = os.path.join(MODEL_DIR, "best_weights_model.keras")
MODEL_PARTS = [
    os.path.join(MODEL_DIR, f"best_weights_model.7z.{str(i).zfill(3)}")
    for i in range(1, 5)
]
model = None

# H√†m h·ª£p nh·∫•t c√°c t·ªáp .7z
def assemble_model_parts():
    try:
        logging.info(f"Assembling model parts: {MODEL_PARTS}")
        with open(ASSEMBLED_MODEL, 'wb') as assembled_file:
            for part in MODEL_PARTS:
                if not os.path.exists(part):
                    logging.error(f"Missing model part: {part}")
                    raise FileNotFoundError(f"Missing part: {part}")
                with open(part, 'rb') as part_file:
                    assembled_file.write(part_file.read())
        logging.info("‚úÖ Successfully assembled model parts into a single .7z file.")
    except Exception as e:
        logging.error(f"‚ùå Failed to assemble model parts: {e}")
        raise

# H√†m gi·∫£i n√©n t·ªáp .7z
def extract_model():
    try:
        logging.info(f"Extracting model from {ASSEMBLED_MODEL}")
        with py7zr.SevenZipFile(ASSEMBLED_MODEL, mode='r') as archive:
            archive.extractall(path=MODEL_DIR)
        logging.info("‚úÖ Successfully extracted model .keras file.")
    except Exception as e:
        logging.error(f"‚ùå Failed to extract model: {e}")
        raise

# H√†m chu·∫©n b·ªã m√¥ h√¨nh
def prepare_model():
    try:
        if not os.path.exists(MODEL_PATH):
            if not os.path.exists(ASSEMBLED_MODEL):
                assemble_model_parts()
            extract_model()
    except Exception as e:
        logging.error(f"‚ùå Error in prepare_model: {e}")
        raise

# H√†m t·∫£i m√¥ h√¨nh v√†o b·ªô nh·ªõ
def load_model():
    global model
    try:
        prepare_model()
        logging.info("üöÄ Loading model into memory...")
        model = tf.keras.models.load_model(MODEL_PATH)
        logging.info("‚úÖ Model loaded successfully!")
    except Exception as e:
        logging.error(f"‚ùå Error loading model: {e}", exc_info=True)
        raise

# T·∫£i m√¥ h√¨nh khi kh·ªüi ƒë·ªông ·ª©ng d·ª•ng
try:
    load_model()
except Exception as e:
    logging.error(f"‚ùå Model initialization failed: {e}")

# Route home
@app.route('/')
def home():
    try:
        logging.info("Rendering index.html for the home route.")
        return render_template('index.html')
    except Exception as e:
        logging.error(f"‚ùå Error rendering home page: {e}", exc_info=True)
        return jsonify({'error': 'Internal server error', 'details': str(e)}), 500

# Route dashboard
@app.route('/dashboard')
def dashboard():
    try:
        logging.info("Rendering dashboard.html for the dashboard route.")
        return render_template('dashboard.html')
    except Exception as e:
        logging.error(f"‚ùå Error rendering dashboard page: {e}", exc_info=True)
        return jsonify({'error': 'Internal server error', 'details': str(e)}), 500

# Route ki·ªÉm tra tr·∫°ng th√°i server
@app.route('/ping', methods=['GET'])
def ping():
    return jsonify({'message': 'Server is running!'}), 200

# Route ki·ªÉm tra tr·∫°ng th√°i m√¥ h√¨nh
@app.route('/model-status', methods=['GET'])
def model_status():
    try:
        if model is not None:
            return jsonify({'status': 'Model is loaded successfully'}), 200
        else:
            return jsonify({'status': 'Model is not loaded'}), 503
    except Exception as e:
        logging.error(f"‚ùå Error checking model status: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500

# Route d·ª± ƒëo√°n
@app.route('/predict', methods=['POST'])
def predict():
    try:
        if model is None:
            logging.warning("Model is not loaded in memory, attempting to load model.")
            load_model()

        # Ki·ªÉm tra file ·∫£nh trong request
        if 'image' not in request.files:
            logging.error("No image file found in the request.")
            return jsonify({'error': 'No image file provided!'}), 400

        file = request.files['image']
        if file.filename == '':
            logging.error("Empty filename provided in the request.")
            return jsonify({'error': 'Empty filename!'}), 400

        # X·ª≠ l√Ω ·∫£nh
        logging.info("Processing image for prediction...")
        img = Image.open(file).convert('RGB').resize((224, 224))
        img_array = np.expand_dims(np.array(img) / 255.0, axis=0)
        img.close()

        # Th·ª±c hi·ªán d·ª± ƒëo√°n
        logging.info("Making prediction with the model...")
        preds = model.predict(img_array)[0][0]
        classification = 'Nodule' if preds > 0.5 else 'Non-Nodule'
        logging.info(f"‚úÖ Prediction successful: Class - {classification}, Score - {preds}")
        return jsonify({'classification': classification, 'score': float(preds)})

    except Exception as e:
        logging.error(f"‚ùå Prediction error: {e}", exc_info=True)
        return jsonify({'error': f'Internal Server Error: {str(e)}'}), 500

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))  # S·ª≠ d·ª•ng c·ªïng t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    app.run(host='0.0.0.0', port=port)
