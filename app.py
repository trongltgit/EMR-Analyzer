import os
import numpy as np
import tensorflow as tf
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from PIL import Image
import logging
import py7zr
import glob

logging.basicConfig(level=logging.DEBUG, filename="server.log",
                    format="%(asctime)s - %(levelname)s - %(message)s")

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Gi·ªõi h·∫°n 16MB
CORS(app, resources={r"/*": {"origins": ["https://emr-analyzer.onrender.com", "http://localhost:3000", "http://localhost:5000"]}})

# Config model
MODEL_DIR = "./models"
MODEL_PATH = os.path.join(MODEL_DIR, "best_weights_model.keras")

model = None

def load_model():
    global model
    if model is None:
        try:
            if not os.path.exists(MODEL_PATH):
                logging.info("Model file not found, preparing to extract...")
                # T√¨m t·∫•t c·∫£ c√°c t·ªáp chia nh·ªè
                split_files = sorted(glob.glob(os.path.join(MODEL_DIR, "best_weights_model.7z.*")))
                if not split_files:
                    logging.error("No split archive files found.")
                    raise FileNotFoundError("No split archive files found.")

                # H·ª£p nh·∫•t c√°c t·ªáp chia nh·ªè th√†nh m·ªôt t·ªáp .7z
                archive_path = os.path.join(MODEL_DIR, "best_weights_model.7z")
                with open(archive_path, 'wb') as outfile:
                    for split_file in split_files:
                        with open(split_file, 'rb') as infile:
                            outfile.write(infile.read())
                logging.info("Split archives merged successfully.")

                # Gi·∫£i n√©n t·ªáp .7z
                try:
                    with py7zr.SevenZipFile(archive_path, mode='r') as archive:
                        archive.extractall(path=MODEL_DIR)
                    logging.info("Model extracted successfully.")
                except Exception as e:
                    logging.error(f"Error extracting model: {str(e)}")
                    raise

                # X√≥a t·ªáp .7z sau khi gi·∫£i n√©n (t√πy ch·ªçn)
                os.remove(archive_path)

            logging.info("üì¶ ƒêang t·∫£i model v√†o b·ªô nh·ªõ...")
            model = tf.keras.models.load_model(MODEL_PATH)
            logging.info("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c load!")
        except Exception as e:
            logging.error(f"L·ªói khi t·∫£i model: {str(e)}")
            print(f"L·ªói khi t·∫£i model: {str(e)}")
            raise

with app.app_context():
    try:
        load_model()
        logging.info("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c preload!")
    except Exception as e:
        logging.error(f"L·ªói preload model: {str(e)}")
        print(f"L·ªói preload model: {str(e)}")

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/dashboard')
def dashboard():
    logging.info("Trang dashboard ƒë∆∞·ª£c truy c·∫≠p.")
    return render_template('dashboard.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        if model is None:
            load_model()
            
        if 'image' not in request.files:
            logging.warning("Kh√¥ng c√≥ file ·∫£nh ƒë∆∞·ª£c g·ª≠i!")
            return jsonify({'error': 'Kh√¥ng c√≥ file ·∫£nh ƒë∆∞·ª£c g·ª≠i!'}), 400

        file = request.files['image']
        if file.filename == '':
            logging.warning("T√™n file r·ªóng!")
            return jsonify({'error': 'T√™n file r·ªóng!'}), 400

        if not file.content_type.startswith('image/'):
            logging.warning("File kh√¥ng ph·∫£i ·∫£nh!")
            return jsonify({'error': 'File ph·∫£i l√† ·∫£nh (jpg, png, ...)!'}), 400

        image = Image.open(file).convert('RGB')
        image = image.resize((224, 224))
        img_array = np.array(image) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        predictions = model.predict(img_array)
        logging.info(f"üìä K·∫øt qu·∫£ d·ª± ƒëo√°n: {predictions}")
        return jsonify({'predictions': predictions.tolist()})
    except Exception as e:
        logging.error(f"L·ªói trong route /predict: {str(e)}")
        print(f"L·ªói trong route /predict: {str(e)}")
        return jsonify({'error': f'L·ªói x·ª≠ l√Ω: {str(e)}'}), 500

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
