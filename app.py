import os
import numpy as np
import tensorflow as tf
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from PIL import Image
import gdown
import logging
import subprocess

logging.basicConfig(level=logging.DEBUG, filename="server.log",
                    format="%(asctime)s - %(levelname)s - %(message)s")

app = Flask(__name__)
CORS(app)

# === Config ===
USE_7Z_SPLIT = True  # Náº¿u báº¡n muá»‘n dÃ¹ng file .7z.001 -> .004 thay vÃ¬ Google Drive
MODEL_FILE_ID = "1EpAgsWQSXi7CsUO8mEQDGAJyjdfN0T6n"  # Náº¿u dÃ¹ng Drive
MODEL_FILE_NAME = "best_weights_model.keras"
MODEL_DIR = "./models"
MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE_NAME)

def extract_7z_parts():
    logging.info("ðŸ”§ Äang ná»‘i vÃ  giáº£i nÃ©n cÃ¡c pháº§n .7z...")
    part_files = [f"{MODEL_DIR}/best_weights_model.7z.{str(i).zfill(3)}" for i in range(1, 5)]

    for file in part_files:
        if not os.path.exists(file):
            raise FileNotFoundError(f"Thiáº¿u file: {file}")

    try:
        subprocess.run(["7z", "x", f"{part_files[0]}", f"-o{MODEL_DIR}"], check=True)
        logging.info("âœ… ÄÃ£ giáº£i nÃ©n thÃ nh cÃ´ng .keras tá»« cÃ¡c pháº§n .7z!")
    except subprocess.CalledProcessError as e:
        logging.error(f"Lá»—i khi giáº£i nÃ©n: {e}")
        raise

def download_model():
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR, exist_ok=True)

    if not os.path.isfile(MODEL_PATH):
        if USE_7Z_SPLIT:
            logging.info("ðŸ“¦ DÃ¹ng file .7z chia nhá» Ä‘á»ƒ láº¥y model...")
            extract_7z_parts()
        else:
            logging.info("ðŸ“¥ Äang táº£i model tá»« Google Drive...")
            url = f"https://drive.google.com/uc?id={MODEL_FILE_ID}"
            gdown.download(url, MODEL_PATH, quiet=False)
            logging.info("âœ… ÄÃ£ táº£i model thÃ nh cÃ´ng!")

model = None
def load_model():
    global model
    if model is None:
        try:
            download_model()
            logging.info("ðŸ“¦ Load model vÃ o bá»™ nhá»›...")
            model = tf.keras.models.load_model(MODEL_PATH)
            logging.info("âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c load!")
        except Exception as e:
            logging.error(f"Lá»—i khi load model: {e}")
            raise

# Preload khi khá»Ÿi Ä‘á»™ng
with app.app_context():
    try:
        load_model()
    except Exception as e:
        logging.error(f"Lá»—i preload model: {e}")

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/dashboard')
def dashboard():
    return render_template('dashboard.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        if model is None:
            load_model()

        if 'image' not in request.files:
            return jsonify({'error': 'KhÃ´ng cÃ³ file áº£nh Ä‘Æ°á»£c gá»­i!'}), 400

        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'TÃªn file rá»—ng!'}), 400

        img = Image.open(file).convert('RGB').resize((224, 224))
        x = np.expand_dims(np.array(img) / 255.0, axis=0)

        preds = model.predict(x)[0][0]
        cls = 'Nodule' if preds > 0.5 else 'Non-Nodule'
        return jsonify({'classification': cls, 'score': float(preds)})
    except Exception as e:
        logging.error(f"Lá»—i trong /predict: {e}")
        return jsonify({'error': f'Internal Server Error: {e}'}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get("PORT", 5000)))
